[
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "Open Source License",
    "section": "",
    "text": "The Quarto source code is available at https://github.com/quarto-dev/\nQuarto is a registered trademark of RStudio. Please see our trademark policy for guidelines on usage of the Quarto trademark.\nQuarto also makes use of several other open-source projects, the distribution of which is subject to their respective licenses. Major components and their licenses include:\n\n\n\nProject\nLicense\n\n\n\n\nPandoc\nGNU GPL v2\n\n\nBootstrap 5.0\nMIT\n\n\nBootswatch 5.0\nMIT\n\n\nDeno\nMIT\n\n\nesbuild\nMIT\n\n\nDart Sass\nMIT\n\n\nObservable Runtime\nISC"
  },
  {
    "objectID": "trademark.html",
    "href": "trademark.html",
    "title": "Trademark Policy",
    "section": "",
    "text": "This policy is adapted directly from the WordPress Foundation’s trademark policy for the WordPress and WordCamp names and logos. We admire the job that WordPress has done building a thriving open source community while at the same time making possible a wide variety of WordPress related businesses. We hope that this policy will help us do the same for Quarto."
  },
  {
    "objectID": "trademark.html#goals",
    "href": "trademark.html#goals",
    "title": "Trademark Policy",
    "section": "Goals",
    "text": "Goals\nRStudio, PBC owns and oversees the trademark for the Quarto name and logo. We have developed this trademark usage policy with the following goals in mind:\n\nWe’d like to make it easy for anyone to use the Quarto name or logo for community-oriented efforts that help spread and improve Quarto.\nWe’d like to make it clear how Quarto-related businesses and projects can (and cannot) use the Quarto name and logo.\nWe’d like to make it hard for anyone to use the Quarto name and logo to unfairly profit from, trick or confuse people who are looking for official Quarto resources.\n\nPlease note that it is not the goal of this policy to limit open source or commercial activity around Quarto. We actively encourage Quarto-based open source projects and businesses—our goal with this policy is to prevent confusion about the source of Quarto related software and services."
  },
  {
    "objectID": "trademark.html#permission",
    "href": "trademark.html#permission",
    "title": "Trademark Policy",
    "section": "Permission",
    "text": "Permission\nPermission from RStudio is required to use the Quarto name or logo as part of any project, product, service, domain name, or company name.\nWe will grant permission to use the Quarto name and logo for projects that meet the following criteria:\n\nThe primary purpose of your project is to promote the spread and improvement of the Quarto software.\nYour project is non-commercial in nature (it can make money to cover its costs or contribute to non-profit entities, but it cannot be run as a for-profit project or business).\nYour project neither promotes nor is associated with entities that currently fail to comply with the GPL license under which Quarto is distributed.\n\nIf your project meets these criteria, you will be permitted to use the Quarto name and logo to promote your project in any way you see fit with these exceptions: (1) Please do not use Quarto as part of a domain name; and (2) We do not allow the use of the trademark in advertising, including AdSense/AdWords.\nAll other Quarto-related businesses or projects can use the Quarto name and logo to refer to and explain their services, but they cannot use them as part of a product, project, service, domain name, or company name and they cannot use them in any way that suggests an affiliation with or endorsement by the Quarto open source project.\nThe abbreviation “QMD” is not covered by the Quarto trademark and you are free to use it in any way you see fit."
  },
  {
    "objectID": "trademark.html#examples",
    "href": "trademark.html#examples",
    "title": "Trademark Policy",
    "section": "Examples",
    "text": "Examples\nA consulting company can describe its business as “123 Publishing Services, offering Quarto consulting for publishers,” but cannot call its business “The Quarto Consulting Company.” Similarly, a business related to Quarto extensions can describe itself as “XYZ Extensions, the world’s best Quarto extensions,” but cannot call itself “The Quarto Extension Portal.”\nSimilarly, it’s OK to use the Quarto logo as part of a page that describes your products or services, but it is not OK to use it as part of your company or product logo or branding itself. Under no circumstances is it permitted to use Quarto as part of a domain name or top-level domain name.\nWhen in doubt about your use of the Quarto name or logo, please contact RStudio at permissions@rstudio.com for clarification."
  },
  {
    "objectID": "documentation/guide/component/creation/nextflow.html",
    "href": "documentation/guide/component/creation/nextflow.html",
    "title": "Nextflow",
    "section": "",
    "text": "This guide explains how to create a Viash component from scratch that targets the Nextflow platform. For a general overview of Viash components, take a look at the Viash Basics guide."
  },
  {
    "objectID": "documentation/guide/component/creation/nextflow.html#creating-a-viash-component",
    "href": "documentation/guide/component/creation/nextflow.html#creating-a-viash-component",
    "title": "Nextflow",
    "section": "Creating a Viash component",
    "text": "Creating a Viash component\n\nAdding a Viash config\ncreate folder named nextflow_component, add src/remove_comments dir create config.vsh.yaml and add to src/remove_comments\nadd this\nfunctionality:\n  name: remove_comments\n  arguments:\n    - name: \"--input\"\n      alternatives: [ \"-i\" ]\n      type: file\n      required: true\n      example: \"file.tsv\"\n    - name: \"--output\"\n      alternatives: [ \"-o\" ]\n      type: file\n      required: true\n      direction: output\n      example: \"file.tsv\"\n  resources:\n  - type: bash_script\n    path: ./script.sh\nplatforms:\n  - type: nextflow\n    variant: vdsl3\n    directives:\n      container: ubuntu:20.04\nshort description of what this does\nfocus on platforms section, explain vdsl3 and directives\nsingle resource needed, a script\n\n\nWriting the script\nadd simple script\n#!/bin/bash\n\ngrep -v '^#' \"$par_input\" > \"$par_output\"\nyou now have this structure which makes up a component\nnextflow_component\n└── src\n    └── remove_comments\n        ├── config.vsh.yaml\n        └── script.sh"
  },
  {
    "objectID": "documentation/guide/component/creation/nextflow.html#running-the-viash-component",
    "href": "documentation/guide/component/creation/nextflow.html#running-the-viash-component",
    "title": "Nextflow",
    "section": "Running the Viash component",
    "text": "Running the Viash component\nIn contrast to native and Docker based components, a Nextflow based component cannot be ran by using the viash run command. These types of components require you to use viash build first to generate a Nextflow module and run that module using nextflow run. See Building and Running Nextflow for more information."
  },
  {
    "objectID": "documentation/guide/component/creation/native.html",
    "href": "documentation/guide/component/creation/native.html",
    "title": "Native",
    "section": "",
    "text": "BashC#JavascriptPythonScalaR\n\n\necho \"Hello Bash\"\n\n\necho \"Hello C#\"\n\n\necho \"Hello Javascript\"\n\n\necho \"Hello Python\"\n\n\necho \"Hello Scala\"\n\n\necho \"Hello R\""
  },
  {
    "objectID": "documentation/guide/component/creation/docker.html",
    "href": "documentation/guide/component/creation/docker.html",
    "title": "Docker",
    "section": "",
    "text": "BashC#JavascriptPythonScalaR\n\n\necho \"Hello Bash\"\n\n\necho \"Hello C#\"\n\n\necho \"Hello Javascript\"\n\n\necho \"Hello Python\"\n\n\necho \"Hello Scala\"\n\n\necho \"Hello R\""
  },
  {
    "objectID": "documentation/guide/data-workflow/nextflow-pipeline/pipeline-basics.html",
    "href": "documentation/guide/data-workflow/nextflow-pipeline/pipeline-basics.html",
    "title": "Pipeline Basics",
    "section": "",
    "text": "explain this guide will go over the basics of VDSL3 and how to use modules inside of a Nextflow pipeline reference to the nextflow website for the basics of nextflow’s Groovy syntax"
  },
  {
    "objectID": "documentation/guide/data-workflow/nextflow-pipeline/pipeline-basics.html#vdsl3",
    "href": "documentation/guide/data-workflow/nextflow-pipeline/pipeline-basics.html#vdsl3",
    "title": "Pipeline Basics",
    "section": "VDSL3",
    "text": "VDSL3\nexplain what VDSL3 is and how it builds upon"
  },
  {
    "objectID": "documentation/guide/data-workflow/nextflow-pipeline/pipeline-basics.html#creating-the-module",
    "href": "documentation/guide/data-workflow/nextflow-pipeline/pipeline-basics.html#creating-the-module",
    "title": "Pipeline Basics",
    "section": "Creating the module",
    "text": "Creating the module\nNextflow works with modules to run scripts and handle their input and output, so the first step is generating a Nextflow module from a Viash component\n\nCreating the component\ncreate a new folder named basic_pipeline, add a new folder to it named src download the remove_comments component files and add them to src\nDownload config.vsh.yaml\nDownload script.sh\n\n\nBuilding the Nextflow module\nviash build src/remove_comments/config.vsh.yaml -o target/remove_comments"
  },
  {
    "objectID": "documentation/guide/data-workflow/nextflow-pipeline/pipeline-basics.html#creating-the-pipeline",
    "href": "documentation/guide/data-workflow/nextflow-pipeline/pipeline-basics.html#creating-the-pipeline",
    "title": "Pipeline Basics",
    "section": "Creating the pipeline",
    "text": "Creating the pipeline\nAdd main.nf:\ntargetDir = \"./target\" // 1\n\ninclude { remove_comments } from \"$targetDir/remove_comments/main.nf\" // 2\n\nworkflow {\n  Channel.fromPath(params.input) // 3\n    | map{ file -> [ file.baseName, file ] } // 4\n    | view{ file -> \"Input: $file\" } // 5\n    | remove_comments.run( // 6\n      auto: [ publish: true ]\n      )\n    | view{ file -> \"Output: $file\" } // 7\n}\nHere’s an overview of this Nextflow script:\n\ntarget dir where the modules are located\ninclude the remove_comments module from the remove_comments/main.nf script\nCreate a channel based on the input parameter’s path\nTake the tuple list and map it to the [ file.baseName, file ] fornmat\nPrint the input tuple to the console\nRun the remove_comments module with auto publishing enabled using the auto directive\nPrint the output tuple to the console\n\nrun the pipeline\nnextflow run main.nf --input \"data/sample.tsv\" --publishDir output\nN E X T F L O W  ~  version 22.04.3\nLaunching `main.nf` [curious_gates] DSL2 - revision: 3e22e3038c\nexecutor >  local (1)\n[2a/5df658] process > remove_comments:remove_comments_process (1) [100%] 1 of 1 ✔\nInput: [sample, /home/blackdragonbe/GitHub/new_website/basic_pipeline/data/sample.tsv]\nOutput: [sample, /home/blackdragonbe/GitHub/new_website/basic_pipeline/work/2a/5df6584524e26995953a4eaec97136/sample.remove_comments.output.tsv]"
  },
  {
    "objectID": "documentation/guide/data-workflow/nextflow-pipeline/pipeline-basics.html#whats-next",
    "href": "documentation/guide/data-workflow/nextflow-pipeline/pipeline-basics.html#whats-next",
    "title": "Pipeline Basics",
    "section": "What’s next?",
    "text": "What’s next?\nthe pipeline in this guide was the bare minimum to learn more about\nAdvanced Pipeline"
  },
  {
    "objectID": "documentation/guide/data-workflow/nextflow-pipeline/pipeline-advanced.html",
    "href": "documentation/guide/data-workflow/nextflow-pipeline/pipeline-advanced.html",
    "title": "Advanced Pipeline",
    "section": "",
    "text": "explain this pipeline is a more realistic example of a typical use-case of a Nextflow bioinformatics pipeline as it has a mixture of scripting languages used and a join is used to merge all events"
  },
  {
    "objectID": "documentation/guide/data-workflow/nextflow-pipeline/pipeline-advanced.html#creating-the-modules",
    "href": "documentation/guide/data-workflow/nextflow-pipeline/pipeline-advanced.html#creating-the-modules",
    "title": "Advanced Pipeline",
    "section": "Creating the modules",
    "text": "Creating the modules\nThis pipeline uses three Nextflow modules which you’ll generate using Viash components:\n\nremove_comments\ntake_columns\ncombine_columns\n\nThe sections below describe how to create these in preparation for the pipeline.\ncreate a new folder named\n\nCreating the remove_comments component\nThis component removes all comments (lines starting with a hashtag) from a tsv file.\nDownload config.vsh.yaml\n\n\n\nContents of config.vsh.yaml\n\nfunctionality:\n  name: remove_comments\n  namespace: nextflow_modules\n  arguments:\n    - name: \"--input\"\n      alternatives: [ \"-i\" ]\n      type: file\n      required: true\n      example: \"file.tsv\"\n    - name: \"--output\"\n      alternatives: [ \"-o\" ]\n      type: file\n      required: true\n      direction: output\n      example: \"file.tsv\"\n  resources:\n  - type: bash_script\n    path: ./script.sh\nplatforms:\n  - type: nextflow\n    variant: vdsl3\n    directives:\n      container: ubuntu:20.04\n\nDownload script.sh\n\n\n\nContents of script.sh\n\n#!/bin/bash\n\ngrep -v '^#' \"$par_input\" > \"$par_output\"\n\n\n\nCreating the take_column component\nThis component subsets an incoming tsv file by extracting a certain column from the file.\nDownload config.vsh.yaml\n\n\n\nContents of config.vsh.yaml\n\nfunctionality:\n  name: take_column\n  namespace: nextflow_modules\n  arguments:\n    - name: \"--input\"\n      alternatives: [ \"-i\" ]\n      type: file\n      required: true\n    - name: \"--output\"\n      alternatives: [ \"-o\" ]\n      type: file\n      required: true\n      direction: output\n    - name: \"--column\"\n      type: integer\n      required: false\n      default: 2\n  resources:\n  - type: python_script\n    path: ./script.py\nplatforms:\n  - type: nextflow\n    variant: vdsl3\n    directives:\n      container: amancevice/pandas:slim\n\nDownload script.py\n\n\n\nContents of script.py\n\nimport pandas as pd\n\n## VIASH START\npar = {\n    \"input\": \"data/file1.tsv\",\n    \"column\": 2,\n    \"output\": \"temp/foo\"\n}\n## VIASH END\n\n# read the tsv file\ntab = pd.read_csv(par[\"input\"], sep=\"\\t\", comment=\"#\")\n\n# subset a column\ntab_filt = tab.iloc[:, par[\"column\"]-1]\n\n# write to file\ntab_filt.to_csv(par[\"output\"], index=False)\n\n\n\nCreating the combine_columns component\nThis component combines multiple tsv files into one by concatenating all of the columns together. It assumes each incoming tsv file has an equal number of rows.\nDownload config.vsh.yaml\n\n\n\nContents of config.vsh.yaml\n\nfunctionality:\n  name: combine_columns\n  namespace: nextflow_modules\n  arguments:\n    - name: \"--input\"\n      alternatives: [ \"-i\" ]\n      type: file\n      multiple: true\n      required: true\n    - name: \"--output\"\n      alternatives: [ \"-o\" ]\n      type: file\n      required: true\n      direction: output\n  resources:\n    - type: r_script\n      path: ./script.R\nplatforms:\n  - type: nextflow\n    variant: vdsl3\n    directives:\n      container: rocker/r-ver:4.1\n\nDownload script.R\n\n\n\nContents of script.R\n\n## VIASH START\npar <- list(\n    input = c(\"data/file1.tsv\", \"data/file2.tsv\"),\n    output = \"temp/foo.tsv\"\n)\n## VIASH END\n\nouts <- lapply(par$input, function(file) {\n  read.delim(file, comment.char = \"#\", sep = \"\\t\", header = FALSE)\n})\n\ntable <- do.call(cbind, outs)\n\nwrite.table(table, par$output, col.names = FALSE, sep = \"\\t\")"
  },
  {
    "objectID": "documentation/guide/data-workflow/nextflow-pipeline/pipeline-advanced.html#building-the-modules",
    "href": "documentation/guide/data-workflow/nextflow-pipeline/pipeline-advanced.html#building-the-modules",
    "title": "Advanced Pipeline",
    "section": "Building the modules",
    "text": "Building the modules\nThe basic pipeline guide describes how to generate an individual VDSL3 Nextflow module using the viash build command, but there’s a better way when it comes to building multiple modules at once: viash ns build.\nviash ns build"
  },
  {
    "objectID": "documentation/guide/building-block/nextflow/building-running.html",
    "href": "documentation/guide/building-block/nextflow/building-running.html",
    "title": "Building and Running",
    "section": "",
    "text": "This guide covers how you can can build a Nextflow module to use in your pipeline."
  },
  {
    "objectID": "documentation/guide/building-block/nextflow/building-running.html#building-a-nextflow-vdsl3-module",
    "href": "documentation/guide/building-block/nextflow/building-running.html#building-a-nextflow-vdsl3-module",
    "title": "Building and Running",
    "section": "Building a Nextflow VDSL3 module",
    "text": "Building a Nextflow VDSL3 module\nTo start with, create a Viash component that targets the Nextflow platform as explained in this guide.\nNext, use the viash build command to generate a Nextflow module inside of a target/remove_comments directory:\nviash build src/remove_comments/config.vsh.yaml -o target/remove_comments\nThis will generate two files in the target/remove_comments directory: main.nf and nextflow.config.\nnextflow_component\n├── src\n│   └── remove_comments\n│       ├── config.vsh.yaml\n│       └── script.sh\n└── target\n    └── remove_comments\n        ├── main.nf\n        └── nextflow.config"
  },
  {
    "objectID": "documentation/guide/building-block/nextflow/building-running.html#runing-a-standalone-pipeline",
    "href": "documentation/guide/building-block/nextflow/building-running.html#runing-a-standalone-pipeline",
    "title": "Building and Running",
    "section": "Runing a standalone pipeline",
    "text": "Runing a standalone pipeline\nA VDSL3 module can be run as a small, standalone Nextflow pipeline.\n\nDocumentation\nIt’s often useful to know what arguments a module expects before trying to run it. To display the documentation for a module, run the module with just the --help argument:\nnextflow run target/remove_comments/main.nf --help\nThis will result in output that looks similar to this:\nN E X T F L O W  ~  version 22.04.3\nLaunching `target/remove_comments/main.nf` [admiring_carlsson] DSL2 - revision: 8de4c7c8eb\nremove_comments\n\nOptions:\n    --input\n        type: file, required parameter\n        example: file.tsv\n\n    --output\n        type: file, required parameter, output\n        example: file.tsv\nAs you can see, this module needs a tsv file as its input and it will write the updated file away to a given output path.\n\n\nRunning the module\nTo test out the module by itself, you need a tsv file to remove the comments from. You can download a sample file we provided here:\n\nDownload sample.tsv\nHere are its contents:\n# this is a header      \n# this is also a header     \none     0.11    123\ntwo     0.23    456\nthree   0.35    789\nfour    0.47    123\nPlace sample.tsv file in a new directory named data, alongside the src and target directories.\nnextflow_component\n├── data\n│   └── sample.tsv\n├── src\n│   └── remove_comments\n│       ├── config.vsh.yaml\n│       └── script.sh\n└── target\n    └── remove_comments\n        ├── main.nf\n        └── nextflow.config\nNow run the module along with the required arguments:\nnextflow run target/remove_comments/main.nf \\\n   --input \"data/sample.tsv\" \\\n   --publishDir output\nBelow is the result:\nN E X T F L O W  ~  version 22.04.3\nLaunching `target/remove_comments/main.nf` [desperate_fermat] DSL2 - revision: 8de4c7c8eb\nWARN: Key for module 'remove_comments' is duplicated.\n\nexecutor >  local (1)\n[2b/17d7a5] process > remove_comments:remove_comments_process1 [100%] 1 of 1 ✔\ninput: [run, [input:/home/user/nextflow_component/data/sample.tsv]]\noutput: [run, /home/user/nextflow_component/work/2b/17d7a50e74d27bc8fd39f098ba06fc/run.remove_comments.output.tsv]\nA new tsv file should’ve been generated now in the output folder named run.remove_comments.output.tsv."
  },
  {
    "objectID": "documentation/guide/building-block/nextflow/building-running.html#whats-next",
    "href": "documentation/guide/building-block/nextflow/building-running.html#whats-next",
    "title": "Building and Running",
    "section": "What’s next?",
    "text": "What’s next?\nto use generated modules inside an actual Nextflow pipeline, see Pipeline Basics"
  },
  {
    "objectID": "documentation/guide/temp/nextflow-basics.html",
    "href": "documentation/guide/temp/nextflow-basics.html",
    "title": "Nextflow Basics",
    "section": "",
    "text": "Nextflow is a popular DSL-based workflow manager with exemplary portability, reproducibility and scalability:\n\nPortability means that one can write the pipeline once, using the Nextflow DSL and run it on different platforms, i.e. what Nextflow calls executors ranging from a simple laptop to cluster systems running hundreds or thousands of nodes.\nDepending on the executor, computations can be scaled out to multiple nodes automatically\nNextflow automatically keeps a log of parameters and intermediate results providing a foundation for reproducible workflows."
  },
  {
    "objectID": "documentation/guide/temp/nextflow-basics.html#nextflow-dialects",
    "href": "documentation/guide/temp/nextflow-basics.html#nextflow-dialects",
    "title": "Nextflow Basics",
    "section": "Nextflow dialects",
    "text": "Nextflow dialects\nOver the past year, the flexibility of the Nextflow DSL has evolved significantly.\n\nDSL1: The original Nextflow syntax, using a more ‘imperative’ programming approach.\nDSL2: A set of built-in helper classes which allow pipelines to be built in more ‘functional’ way.\nThe nf-core project uses a combination of DSL2 and custom Groovy helper classes to reduce some of the boilerplate code.\nVDSL3: An extension of DSL2 which uses to Viash to make Nextflow functional and more ‘batteries included’, see also VDSL3."
  },
  {
    "objectID": "documentation/guide/temp/realistic-pipeline.html",
    "href": "documentation/guide/temp/realistic-pipeline.html",
    "title": "Realistic Nextflow Pipeline",
    "section": "",
    "text": "This version of the pipeline represents a more realistic use-case of typical Nextflow bioinformatics pipelines.\n\nIt has a mixture of Bash, Python and R components\nThe data flow now also features a join, where all events are merged into a single event.\n\nBefore showing the new Nextflow pipeline, let’s first introduce the Python and R components.\n\n\nThis component subsets an incoming tsv file by extracting a certain column from the file. The take_column component is similar to the take_second_column component from the previous section, except we converted it to Python and made the column an optional parameter.\nContents of src/vdsl3_tutorial/take_column/:\n\n\nconfig.vsh.yaml\n\nfunctionality:\n  name: take_column\n  namespace: vdsl3_tutorial\n  arguments:\n    - name: \"--input\"\n      alternatives: [ \"-i\" ]\n      type: file\n      required: true\n    - name: \"--output\"\n      alternatives: [ \"-o\" ]\n      type: file\n      required: true\n      direction: output\n    - name: \"--column\"\n      type: integer\n      required: false\n      default: 2\n  resources:\n  - type: python_script\n    path: ./script.py\nplatforms:\n  - type: nextflow\n    variant: vdsl3\n    directives:\n      container: amancevice/pandas:slim\n\n\n\nscript.py\n\nimport pandas as pd\n\n## VIASH START\npar = {\n    \"input\": \"data/file1.tsv\",\n    \"column\": 2,\n    \"output\": \"temp/foo\"\n}\n## VIASH END\n\n# read the tsv file\ntab = pd.read_csv(par[\"input\"], sep=\"\\t\", comment=\"#\")\n\n# subset a column\ntab_filt = tab.iloc[:, par[\"column\"]-1]\n\n# write to file\ntab_filt.to_csv(par[\"output\"], index=False)\n\n\n\n\nThis component combines multiple tsv files into one by concatenating all of the columns together. It assumes each incoming tsv file has an equal number of rows.\nContents of src/vdsl3_tutorial/combine_columns/:\n\n\nconfig.vsh.yaml\n\nfunctionality:\n  name: combine_columns\n  namespace: vdsl3_tutorial\n  arguments:\n    - name: \"--input\"\n      alternatives: [ \"-i\" ]\n      type: file\n      multiple: true\n      required: true\n    - name: \"--output\"\n      alternatives: [ \"-o\" ]\n      type: file\n      required: true\n      direction: output\n  resources:\n    - type: r_script\n      path: ./script.R\nplatforms:\n  - type: nextflow\n    variant: vdsl3\n    directives:\n      container: rocker/r-ver:4.1\n\n\n\nscript.R\n\n## VIASH START\npar <- list(\n    input = c(\"data/file1.tsv\", \"data/file2.tsv\"),\n    output = \"temp/foo.tsv\"\n)\n## VIASH END\n\nouts <- lapply(par$input, function(file) {\n  read.delim(file, comment.char = \"#\", sep = \"\\t\", header = FALSE)\n})\n\ntable <- do.call(cbind, outs)\n\nwrite.table(table, par$output, col.names = FALSE, sep = \"\\t\")\n\n\n\n\nIn the previous sections, we created three Viash components.\nContents of src/vdsl3_tutorial/:\nsrc/vdsl3_tutorial/\n├── combine_columns\n│   ├── config.vsh.yaml\n│   └── script.R\n├── remove_comments\n│   ├── config.vsh.yaml\n│   └── script.sh\n└── take_column\n    ├── config.vsh.yaml\n    └── script.py\n\n3 directories, 6 files\nRather than generate VDSL3 modules from each Viash component individually using the viash build command, we can build all of the components in a namespace (ns) using the viash ns build command in one go. The resulting files will be stored under target/<platform>/<namespace>/<component_name>.\nviash ns build -q vdsl3_tutorial\nExporting take_column (vdsl3_tutorial) =nextflow=> target/nextflow/vdsl3_tutorial/take_column\nExporting combine_columns (vdsl3_tutorial) =nextflow=> target/nextflow/vdsl3_tutorial/combine_columns\nExporting remove_comments (vdsl3_tutorial) =nextflow=> target/nextflow/vdsl3_tutorial/remove_comments\nThe target directory now contains our Nextflow modules:\nContents of target:\ntarget/\n└── nextflow\n    └── vdsl3_tutorial\n        ├── combine_columns\n        │   ├── main.nf\n        │   └── nextflow.config\n        ├── remove_comments\n        │   ├── main.nf\n        │   └── nextflow.config\n        └── take_column\n            ├── main.nf\n            └── nextflow.config\n\n5 directories, 6 files\n\n\n\nDuring a more complicated Nextflow workflow, it can be useful to keep track of what transformations are being performed at critical steps of the pipeline. In this case, we simply labelled all steps.\nContents of workflows/310-realistic_pipeline/main.nf:\nnextflow.enable.dsl=2\n\ntargetDir = \"../../target/nextflow\"\n\ninclude { remove_comments } from \"$targetDir/vdsl3_tutorial/remove_comments/main.nf\"\ninclude { take_column } from \"$targetDir/vdsl3_tutorial/take_column/main.nf\"\ninclude { combine_columns } from \"$targetDir/vdsl3_tutorial/combine_columns/main.nf\"\n\nworkflow {\n  Channel.fromPath(params.input)\n  \n    // Assign unique ID to each event\n    //   File -> (String, File)\n    | map{ file -> [ file.baseName, file ] }\n    \n    // Remove comments from TSV\n    //   (String, File) -> (String, File)\n    | remove_comments\n\n    // Extract single column from TSV\n    //   (String, File) -> (String, File)\n    | take_column\n\n    // Combine all events into a single List event\n    //   (String, File)* -> List[(String, File)]\n    | toList()\n\n    // Add unique ID to tuple\n    //   List[(String, File)] -> (String, {input: List[File]})\n    | map{ tups -> \n      files = tups.collect{id, file -> file}\n      [ \"combined\", [ input: files ] ] \n    }\n\n    // Concatenate TSVs into one\n    //   (String, {input: List[File]}) -> (String, File)\n    | combine_columns\n\n    // View channel contents\n    | view{ file -> \"Output: $file\" }\n}\nIn terms of a marble diagram, the toList() operator can be represented as in Figure 1.\n\n\n\nFigure 1: The data flow of the toList() part of workflows/310-realistic_pipeline/main.nf.\n\n\nWe are dealing with tuples in VDSL3 IO, so the above would turn into:\n\n\n\nFigure 2: The data flow of the toList() in combination with tuples.\n\n\n\n\n\nWhen we run the updated pipeline, we can see that the execution seems to run correctly.\nnextflow run workflows/310-realistic_pipeline/main.nf \\\n  --input \"data/file?.tsv\" --publishDir output\nN E X T F L O W  ~  version 22.04.3\nLaunching `workflows/310-realistic_pipeline/main.nf` [peaceful_kare] DSL2 - revision: 65be946be4\n[26/03f3d6] Submitted process > remove_comments:remove_comments_process (2)\n[39/b4aecb] Submitted process > remove_comments:remove_comments_process (1)\n[e9/0e5f7f] Submitted process > take_column:take_column_process (1)\n[cc/f65b62] Submitted process > take_column:take_column_process (2)\n[47/12ee1c] Submitted process > combine_columns:combine_columns_process\nOutput: [combined, /home/runner/work/viash_nxf_course/viash_nxf_course/work/47/12ee1cf9a37c5450db605199c8abf7/combined.combine_columns.output]\nEach process seems to have finished correctly (exit code 0).\nHowever, no output was created in the output directory. In fact, the output directory wasn’t even created when running the pipeline.\ntree output\noutput [error opening dir]\n\n0 directories, 0 files\nThe reason for this is that VDSL3 modules are meant to be reusable components. However, at the time of writing the Viash component (i.e. the config.vsh.yaml) you might not know whether the component is the last step of the pipeline, so it does not make sense to decide whether a component’s output will be published or not."
  },
  {
    "objectID": "documentation/guide/temp/realistic-pipeline.html#benefits-of-vdsl3",
    "href": "documentation/guide/temp/realistic-pipeline.html#benefits-of-vdsl3",
    "title": "Realistic Nextflow Pipeline",
    "section": "Benefits of VDSL3",
    "text": "Benefits of VDSL3\nThis guide introduced some key concepts of VDSL3 modules. In comparison to standard Nextflow modules, a VDSL3 module offers:\n\nReadable but flexible pipelines\nAutomatic file naming to avoid naming conflicts\nFlexible directives\nEasy polylingualism (integrates well with custom Docker container builds, see next guide)\nFree standalone CLI and --help\nOverridable arguments with defaults\n\nOne limitation of VDSL3 is that Nextflow processes are able to accept tuples of any format whatsoever, whereas VDLS3 modules have a more rigid interface (see appendix). However, we argue that this limitation is actually a benefit, as it makes modules more interchangeable and have argument defaults.\nIn the next guide, we’ll see that generating VDSL3 modules is actually just one aspect of Viash."
  },
  {
    "objectID": "documentation/guide/temp/adapting-modules.html",
    "href": "documentation/guide/temp/adapting-modules.html",
    "title": "Adapting Modules",
    "section": "",
    "text": "One major benefit of VDSL3 modules in comparison to standard Nextflow processes is that the directives (e.g. whether or not to publish output files) can be defined dynamically using the .run() function.\n\n\nTo actually publish the results, we need to define a publishDir directive. Additionally, we also set specific memory and cpu usage requirements per process.\n\n\n<...>\n  // Concatenate TSVs into one\n  //   (String, {input: List[File]}) -> (String, File)\n  | combine_columns\n\n\n\n<...>\n  // Concatenate TSVs into one\n  //   (String, {input: List[File]}) -> (String, File)\n  | combine_columns.run(\n    directives: [\n      publishDir: params.publishDir,\n      cpus: 4,\n      memory: \"10G\"\n    ]\n  )\n\n\n\n\nThe .run() function also provides some functionality for modifying incoming events. For example, you can change the value of a parameter by passing a map to args. More complicated modifications can be found in the appendix.\n\n\n<...>\n  // Extract single column from TSV\n  //   (String, File) -> (String, File)\n  | take_column\n\n\n\n<...>\n  // Extract single column from TSV\n  //   (String, File) -> (String, File)\n  | take_column.run(\n    args: [ column: 1 ]\n  )"
  },
  {
    "objectID": "documentation/guide/temp/simple-pipeline.html",
    "href": "documentation/guide/temp/simple-pipeline.html",
    "title": "DSL2 Pipeline",
    "section": "",
    "text": "Nextflow DSL2 borrows some elements from event-driven functional programming. As a matter of fact, one could argue that Nextflow’s Channel concept being strictly speaking an example of the DataFlow Programming Model can in fact be regarded as an implementation of a (albeit limited) Functional Reactive Programming library.\nContents of workflows/200-first_nextflow_pipeline/main.nf:\nworkflow {\n  Channel.fromList( [\" a \", \"   b\", \"  c\", \"d  \"] )\n    | map{ elem -> elem.trim() }\n    | view\n}\nThis workflow consists of three steps:\n\nAn channel is created containing 4 strings\nA map which removes spaces around the strings (.trim()).\nA view which displays the contents of the events to the user\n\nThe pipe operator (|) allows connecting steps (which might generate and/or consume events) together.\nQuite a lot is going on in these 3 lines of code. Before we dissect this in detail, let us first explore the Channel or usually called (reactive) stream concept.\n\n\nBelow you can see an illustration of how an empty channel can be created and how events can be put on that channel. The technical term for putting events on the channel is bind.\nworkflow {\n  ch = Channel.empty()\n\n  ch << \" a \"\n  ch << \"   b\"\n  ch << \"  c\"\n  ch << \"d \"\n\n  ch\n    | map{ elem -> elem.trim() }\n    | subscribe{ print \"$it\" }\n}\nThis pipeline definition does exactly the same as our previous example and just aims to describe what is happening under the hood. The Channel.fromList() used in the first example is an illustration of a Channel factory method.\nThe data flow of channels (and later processes) can be visualised as shown in Figure 1.\n\n\n\nFigure 1: The data flow of workflows/200-first_nextflow_pipeline/main.nf.\n\n\n\n\n\nNextflow is a DSL on top of the Groovy programming language, so you can use whatever Groovy code to manipulate Channel events in however way you like[^2].\n\n[ 1, 2, 3 ]: A list of integers\n[ a: 1, b: 2, c: 3 ]: A hash map (dictionary, named list)\n{ elem -> elem.trim() }: An anonymous function, aka closure\n{ it.trim() }: The same anonymous function with the implicit variable it\na ? b : c: If a then b else c\n\nHere is a cheat sheet on Groovy syntax.\n\n\n\nLet’s see what happens when we run the pipeline above using Nextflow:\nnextflow run workflows/200-first_nextflow_pipeline/main.nf\nN E X T F L O W  ~  version 22.04.3\nLaunching `workflows/200-first_nextflow_pipeline/main.nf` [evil_pare] DSL2 - revision: cfd75d221a\na\nb\nc\nd"
  },
  {
    "objectID": "documentation/guide/temp/output-naming.html",
    "href": "documentation/guide/temp/output-naming.html",
    "title": "Output Naming",
    "section": "",
    "text": "The default naming convention of output files is '$id.$key.$arg.$ext', where:\n\n$id is the first element of the event tuple.\n$key is the name of the component (e.g. remove_comments). This can be overridden using .run(key: \"foo\").\n$arg is the name of the output argument (e.g. output).\n$ext is an extension extracted from the example or default of the argument, if it exists.\n\nYou can manually specify the output file names by adding it as an argument.\nExample:\n  // Concatenate TSVs into one\n  //   (String, {input: List[File]}) -> (String, File)\n  | combine_columns.run(\n    args: [ output: \"output.tsv\" ]\n  )\nYou can use a map to ensure that the output name does make use of the id:\n  // Remove comments from TSV\n  //   (String, File) -> (String, File)\n  | remove_comments.run(\n    map: { tup -> [ tup[0], [ input: tup[1], output: \"output_${tup[0]}.tsv\" ] ] }\n  )"
  },
  {
    "objectID": "documentation/guide/temp/vdsl3-basics.html",
    "href": "documentation/guide/temp/vdsl3-basics.html",
    "title": "VDSL3 Basics",
    "section": "",
    "text": "At one point, we (at Data Intuitive) thought about what would make for an “ideal” Nextflow module (cfr. DIFlow by Toni Verbeiren). We found out that, in order for the Nextflow module to have the properties that we wanted, the component would need to contain a lot of boilerplate code.\nLuckily, Viash can be used to generate the boilerplate code for you. To do so, we just need to take our script and add some metadata to it.\n\nViash allows easy prototyping of reusable pipeline components. From Cannoodt et al. 2021 arXiv:2110.11494.\nIn this section, we will use Viash to generate Nextflow modules that behave similarly to the ones created in the previous section, but with some distinct advantages. Since we believe that these Nextflow modules are so different from the original Nextflow DSL2 processes and workflows, we named them Viash DSL3 modules, or VDSL3 modules for short."
  },
  {
    "objectID": "documentation/guide/temp/vdsl3-basics.html#how-to-create-and-use-a-vdsl3-module",
    "href": "documentation/guide/temp/vdsl3-basics.html#how-to-create-and-use-a-vdsl3-module",
    "title": "VDSL3 Basics",
    "section": "How to create and use a VDSL3 module",
    "text": "How to create and use a VDSL3 module\nBelow shows a comparison of the original remove_comments process and a Viash component that has the same functionality.\nTODO: ask to create these and explain\nVDSL3 Viash component\n\n\nconfig.vsh.yaml\n\nfunctionality:\n  name: remove_comments\n  namespace: vdsl3_tutorial\n  arguments:\n    - name: \"--input\"\n      alternatives: [ \"-i\" ]\n      type: file\n      required: true\n      example: \"file.tsv\"\n    - name: \"--output\"\n      alternatives: [ \"-o\" ]\n      type: file\n      required: true\n      direction: output\n      example: \"file.tsv\"\n  resources:\n  - type: bash_script\n    path: ./script.sh\nplatforms:\n  - type: nextflow\n    variant: vdsl3\n    directives:\n      container: ubuntu:20.04\n\n\n\nscript.sh\n\n#!/bin/bash\n\ngrep -v '^#' \"$par_input\" > \"$par_output\"\n\n\nGenerating a VDSL3 module from a Viash config\nThere are definitely some similarities between the original Nextflow process and the Viash component. To actually make use of the new component inside of our Nextflow pipeline, we first need to export the Viash component into a VDSL3 module by running the following command:\nviash build src/vdsl3_tutorial/remove_comments/config.vsh.yaml \\\n  -o target/nextflow/vdsl3_tutorial/remove_comments\nThis results in the creation of the following files:\ntree target/nextflow/vdsl3_tutorial/remove_comments\ntarget/nextflow/vdsl3_tutorial/remove_comments\n├── main.nf\n└── nextflow.config\n\n0 directories, 2 files\n\n\nRun module as a standalone\nA VDSL3 module can actually already be used as a standalone Nextflow pipeline (albeit a very small one).\nYou can display the documentation using the help argument:\nnextflow run target/nextflow/vdsl3_tutorial/remove_comments/main.nf --help\nN E X T F L O W  ~  version 22.04.3\nLaunching `target/nextflow/vdsl3_tutorial/remove_comments/main.nf` [distracted_stallman] DSL2 - revision: 8de4c7c8eb\nremove_comments\n\nOptions:\n    --input\n        type: file, required parameter\n        example: file.tsv\n\n    --output\n        type: file, required parameter, output\n        example: file.tsv\nRunning the component results in the following output:\nnextflow run target/nextflow/vdsl3_tutorial/remove_comments/main.nf \\\n  --input \"data/file1.tsv\" \\\n  --publishDir output/\nN E X T F L O W  ~  version 22.04.3\nLaunching `target/nextflow/vdsl3_tutorial/remove_comments/main.nf` [wise_jang] DSL2 - revision: 8de4c7c8eb\nWARN: Key for module 'remove_comments' is duplicated.\n\ninput: [run, [input:/home/runner/work/viash_nxf_course/viash_nxf_course/data/file1.tsv]]\n[7e/8cd91f] Submitted process > remove_comments:remove_comments_process1\noutput: [run, /home/runner/work/viash_nxf_course/viash_nxf_course/work/7e/8cd91f5274d3e46188ec62106d8ecc/run.remove_comments.output.tsv]\n\n\nUse module in a pipeline\nMore importantly, a VDSL3 module can be included as part of a Nextflow pipeline as follows:\nContents of workflows/300-first_vdsl_pipeline/main.nf:\nnextflow.enable.dsl=2\n\ntargetDir = \"../../target/nextflow\"\n\ninclude { remove_comments } from \"$targetDir/vdsl3_tutorial/remove_comments/main.nf\"\n\nworkflow {\n  Channel.fromPath(params.input)\n    | map{ file -> [ file.baseName, file ] }\n    | view{ file -> \"Input: $file\" }\n    | remove_comments\n    | view{ file -> \"Output: $file\" }\n}\n\n\n\n\n\n\nNote\n\n\n\nThe ability to include a module is part of the Nextflow DSL2 functionality.\n\n\nnextflow run workflows/300-first_vdsl_pipeline/main.nf \\\n  --input \"data/file?.tsv\" --publishDir output\nN E X T F L O W  ~  version 22.04.3\nLaunching `workflows/300-first_vdsl_pipeline/main.nf` [lethal_mandelbrot] DSL2 - revision: 1c8e8aacbe\nInput: [file1, /home/runner/work/viash_nxf_course/viash_nxf_course/data/file1.tsv]\nInput: [file2, /home/runner/work/viash_nxf_course/viash_nxf_course/data/file2.tsv]\n[9c/505943] Submitted process > remove_comments:remove_comments_process (1)\n[f4/540a13] Submitted process > remove_comments:remove_comments_process (2)\nOutput: [file1, /home/runner/work/viash_nxf_course/viash_nxf_course/work/9c/505943f6280fd9d054b9937d2ca13e/file1.remove_comments.output.tsv]\nOutput: [file2, /home/runner/work/viash_nxf_course/viash_nxf_course/work/f4/540a13f599819c7681550901544ac9/file2.remove_comments.output.tsv]"
  },
  {
    "objectID": "documentation/guide/temp/tuples.html",
    "href": "documentation/guide/temp/tuples.html",
    "title": "Modifying Tuples",
    "section": "",
    "text": "Viash offers sugar syntax to modify incoming arguments in various ways:\nManual map:\n  // Assign unique ID to each event\n  //   File -> (String, File)\n  | map{ file -> [ file.baseName, file ] }\n  \n  // Remove comments from TSV\n  //   (String, File) -> (String, File)\n  | remove_comments\nMap included in .run():\n  // Remove comments from TSV\n  // Inner map adds unique ID to tuple\n  //   File -> (String, File)\n  | remove_comments.run(\n    map: { file -> [ file.baseName, file ] }\n  )\nPossible tuple modification arguments:\n\nmap: Apply a map over the incoming tuple.\nmapId: Apply a map over the ID element of a tuple (i.e. the first element).\nmapData: Apply a map over the data element of a tuple (i.e. the second element).\nmapPassthrough: Apply a map over the passthrough elements of a tuple (i.e. the tuple excl. the first two elements).\nrenameKeys: Rename keys in the data field of the tuple (i.e. the second element).\nargs: Fixed argument values which will be added to each tuple in the channel.\ndebug: Whether or not to print debug messages.\n\nSetting debug: true will help you understand how events are modified at any stage of your pipeline. Effectively, it triggers a few view() statements throughout the execution of the module.\nprocess 'take_column' input tuple: [file1, work/46/6bffd3749913149f7fcbbe2d1af7c6/file1.remove_comments.output.tsv]\nprocess 'take_column' processed tuple: [file1, [input:work/46/6bffd3749913149f7fcbbe2d1af7c6/file1.remove_comments.output.tsv]]\nprocess 'take_column' output tuple: [file1, work/44/c3f2d7dd291ea394b3334f3117b45b/file1.take_column.output]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Quarto",
    "section": "",
    "text": "The overarching goal of Quarto is to make the process of creating and collaborating on scientific and technical documents dramatically better. We hope to do this in several dimensions:\n\nCreate a writing and publishing environment with great integrated tools for technical content. We want to make authoring with embedded code, equations, figures, complex diagrams, interactive widgets, citations, cross references, and the myriad other special requirements of scientific discourse straightforward and productive for everyone.\nHelp authors take full advantage of the web as a connected, interactive platform for communications, while still providing the ability to create excellent printed output from the same document source. Researchers shouldn’t need to choose between LaTeX, MS Word, and HTML but rather be able to author documents that target all of them at the same time.\nMake reproducible research and publications the norm rather than the exception. Reproducibility requires that the code and data required to create a manuscript are an integrated part of it. However, this isn’t often straightforward in practice—Quarto aims to make it easier to adopt a reproducible workflow than not.\n\nQuarto is open source software licensed under the GNU GPL v2. We believe that it’s better for everyone if the tools used for research and science are free and open. Reproducibility, widespread sharing of knowledge and techniques, and the leveling of the playing field by eliminating cost barriers are but a few of the shared benefits of free software in science."
  },
  {
    "objectID": "about.html#project",
    "href": "about.html#project",
    "title": "About Quarto",
    "section": "Project",
    "text": "Project\nAt the core of Quarto is Pandoc, a powerful and flexible document processing tool. Quarto adds a number of facilities to Pandoc aimed at scientific and technical publishing, including:\n\nEmbedding code and output from Python, R, and JavaScript via integration with Jupyter, Knitr, and Observable.\nA variety of extensions to Pandoc markdown useful for technical writing including cross-references, sub-figures, layout panels, hoverable citations and footnotes, callouts, and more.\nA project system for rendering groups of documents at once, sharing options across documents, and producing aggregate output like websites and books.\n\nDevelopment of Quarto is sponsored by RStudio, PBC, where we previously created a similar system (R Markdown) that shared the same goals, but was targeted principally at users of the R language. The same core team works on both Quarto and R Markdown:\n\nJ.J. Allaire (@jjallaire)\nChristophe Dervieux (@cderv)\nCarlos Scheidegger (@cscheid)\nCharles Teague (@dragonstyle)\nYihui Xie (@yihui)\n\nWith Quarto, we are hoping to bring these tools to a much wider audience.\nQuarto is a registered trademark of RStudio. Please see our trademark policy for guidelines on usage of the Quarto trademark."
  },
  {
    "objectID": "about.html#contribute",
    "href": "about.html#contribute",
    "title": "About Quarto",
    "section": "Contribute",
    "text": "Contribute\nYou can contribute to Quarto in many ways:\n\nBy opening issues to provide feedback and share ideas.\nBy submitting Pull Request (PR) to fix opened issues\nBy submitting Pull Request (PR) to suggest new features (it is considered good practice to open an issue for discussion before working on a pull request for a new feature).\n\nPlease be mindful of our code of conduct as you interact with other community members.\n\nPull Requests\nPull requests are very welcome! Here’s how to contribute via PR:\n\nFork the repository, clone it locally, and make your changes in a new branch specific to the PR. For example:\n# clone your fork\n$ git clone https://github.com/<username>/quarto-cli\n\n# configure for your platform (./configure-macos.sh, ./configure-linux.sh, or ./configure-windows.cmd)\n$ cd quarto-cli\n$ ./configure-macos.sh\n\n# checkout a new branch\n$ git checkout -b feature/newthing\nFor significant changes (e.g more than small bug fixes), ensure that you have signed the individual or corporate contributor agreement as appropriate. You can send the signed copy to jj@rstudio.com.\nSubmit the pull request. It is ok to submit as draft in your are still working on it but would like some feedback from us. It always good to share in the open that you are working on it.\n\nWe’ll try to be as responsive as possible in reviewing and accepting pull requests."
  }
]